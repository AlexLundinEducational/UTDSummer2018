Name: Alex Lundin
Assignment: HW2

# Linear Regression
#1.	Load library ISLR. Use the names() function and the summary() function to learn more about the Auto dataFrame1 set. Divide the dataFrame1 randomly into train and test sets, with 75% train. Use seed 1234 for reproducibility.
```{r}


# load dataFrame1
if(!require('ISLR')){
# install dataFrame1 if needed
  #install.packages('ISLR')  
  library('ISLR')
}

# learn about Auto
names(Auto)
summary(Auto)


# store Auto into dataFrame1
dataFrame1 <- (Auto)
numberOfRows <- nrow(dataFrame1)

# Set random seed to ensure reproducability of the shuffle.
set.seed(1234)

# shuffle the dataFrame1 and store into a new dataFrame1 frame
shuffled_dataFrame1 <- dataFrame1[sample(numberOfRows), ]

# set train dataFrame1 with the train indices
train_indices <- 1:round(0.75 * numberOfRows)
train <- shuffled_dataFrame1[train_indices, ]
# set test dataFrame1 with the test indices
test_indices <- (round(0.75 * numberOfRows) + 1):numberOfRows
test <- shuffled_dataFrame1[test_indices, ]

```




#2.	Use the lm() function to perform simple linear regression on the train data with mpg as the response and horsepower as the predictor. Use the summary() function to evaluate the results. Calculate the MSE.

```{r}
# data analysis for understanding
# http://r-statistics.co/Linear-Regression.html

# scatterplot for data view
xPredictor <- train$horsepower
yResponse <- train$mpg
scatter.smooth(x=xPredictor, yResponse, main="MPG ~ Horsepower", xlab="Horsepower", ylab="Mpg")  # scatterplot


# box plot for outliers, anything between 25% and 75% percentiles
par(mfrow=c(1, 2))  # divide graph area in 2 columns
boxplot(xPredictor, main="horsepower", sub=paste("Outlier rows: ", boxplot.stats(xPredictor)$out))  # box plot for 'speed'
boxplot(yResponse, main="MPG", sub=paste("Outlier rows: ", boxplot.stats(yResponse)$out))  # box plot for 'distance'


if(!require("e107")){
# install dataFrame1 if needed
  #install.packages("e1071", dep = TRUE)
  library(e1071)
}

# Density plot
par(mfrow=c(1, 2))  # divide graph area in 2 columns
plot(density(xPredictor), main="Density Plot: Speed", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(xPredictor), 2)))  # density plot for 'speed'
polygon(density(xPredictor), col="red")
plot(density(yResponse), main="Density Plot: Distance", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(yResponse), 2)))  # density plot for 'dist'
polygon(density(yResponse), col="red")


# correlation
cor(xPredictor, yResponse)  # calculate correlation between speed and distance 

# create linear model
linearMod <- lm(horsepower ~ mpg, data=Auto)  # build linear regression model on full data
print(linearMod)
# create summary
summary(linearMod)  # model summary

# calculate p value
#modelSummary <- summary(linearMod)  # capture model summary as an object
#modelCoeffs <- modelSummary$coefficients  # model coefficients

#beta.estimate <- modelCoeffs["horsepower", "Estimate"]  # get beta estimate for speed
#std.error <- modelCoeffs["horsepower", "Std. Error"]  # get std.error for speed
#t_value <- beta.estimate/std.error  # calc t statistic
#p_value <- 2*pt(-abs(t_value), df=nrow(Auto)-ncol(Auto))  # calc p Value
#f_statistic <- linearMod$fstatistic[1]  # fstatistic
#f <- summary(linearMod)$fstatistic  # parameters for model p-value calc
#model_p <- pf(f[1], f[2], f[3], lower=FALSE)



#Step 1: Create the training (development) and test (validation) data samples from original data.
# Create Training and Test data -
set.seed(100)  # setting seed to reproduce results of random sampling
trainingRowIndex <- sample(1:nrow(Auto), 0.8*nrow(Auto))  # row indices for training data
trainingData <- Auto[trainingRowIndex, ]  # model training data
testData  <- Auto[-trainingRowIndex, ]   # test data

#Step 2: Develop the model on the training data and use it to predict the distance on test data
# Build the model on training data -
lmMod <- lm(mpg ~ horsepower, data=trainingData)  # build the model
distPred <- predict(lmMod, testData)  # predict distance


# Step 3: Review diagnostic measures.
summary (lmMod)  # model summary

AIC (lmMod)  # Calculate akaike information criterion



```


#3.	(No code) Write in the white text area: 
#a.	Write the equation from the model  
#b.	Is there a strong relationship between horsepower and mpg? 
#c.	Is it a positive or negative correlation? 
#d.	Comment on the RSE, R^2, and F-statistic 
#e.	Comment on the MSE



#4.	Plot train$mpg~train$horsepower and draw a blue abline(). Comment on how well the data fits the line. Predict mpg for horsepower of 98. Comment on the predicted value given the graph you created.



#5.	Test on the test data using the predict function. Find the correlation between the predicted values and the mpg values in the test data. Comment on the results. Calculate the mse on the test results. Compare this to the mse for the training data. 



#6.	Plot the linear model in a 2x2 arrangement. Do you see evidence of non-linearity from the residuals?



#7.	Create a second linear model with log(mpg) predicted by horsepower. Compare the summary statistic R^2 of the two models. 



#8.	Plot the function with an abline(). Comment on how well the line fits the data.



#9.	Predict on the test data using lm2. Find the correlation of the predictions and log() of test mpg. Compare this correlation with the correlation you got for lm1. Calculate the MSE for the test data on lm2 and compare to lm1.



#10.	Plot the second linear model in a 2x2 arrangement. How does it compare to the first set of graphs?




# Multiple Linear Regression
#1.	Produce a scatterplot matrix which includes all the variables in the data set using the command "pairs(Auto)". List any possible correlations that you observe, listing positive and negative correlations separately, with at least 3 in each category.



#2.	Compute the matrix of correlations between the variables using function cor(). You should exclude the "name" variable since is it qualitative. Write the two strongest positive correlations and their values in the text area. Write the two strongest negative correlations and their values in the text area.



#3.	Convert the origin variable to a factor. Use the lm() function to perform multiple linear regression with mpg as the response and all other variables except name as predictors. Use the summary() function to print the results. Which predictors appear to have a statistically significant relationship to the response?



#4.	Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Are there any leverage points? Display a row from the data set that seems to be a leverage point. 



#5.	Use the * and + symbols to fit linear regression models with interaction effects, choosing whatever variables you think might get better results than your model in step 3 above. Compare the summaries of the two models, particularly R^2. Run anova() on the two models to see if your second model outperformed the previous one. 
