Name: Alex Lundin
Assignment: HW2

# Linear Regression
#1.	Load library ISLR. Use the names() function and the summary() function to learn more about the Auto auto.dataFrame set. Divide the auto.dataFrame randomly into train.data and test.data sets, with 75% train.data. Use seed 1234 for reproducibility.
```{r}


# load auto.dataFrame
if(!require('ISLR')){
# install auto.dataFrame if needed
  #install.packages('ISLR')  
  library('ISLR')
}

# learn about Auto
names(Auto)
summary(Auto)


# store Auto into auto.dataFrame
auto.dataFrame <- (Auto)
auto.numberOfRows <- nrow(auto.dataFrame)

# Set random seed to ensure reproducability of the shuffle.
set.seed(1234)

# shuffle the auto.dataFrame and store into a new auto.dataFrame frame
shuffled_auto.dataFrame <- auto.dataFrame[sample(auto.numberOfRows), ]

# set train.data auto.dataFrame with the train.data indices
train.data.indices <- 1:round(0.75 * auto.numberOfRows)
train.data <- shuffled_auto.dataFrame[train.data.indices, ]
# set test.data auto.dataFrame with the test.data indices
test.data.indices <- (round(0.75 * auto.numberOfRows) + 1):auto.numberOfRows
test.data <- shuffled_auto.dataFrame[test.data.indices, ]

```




#2.	Use the lm() function to perform simple linear regression on the train.data data with mpg as the response and horsepower as the predictor. Use the summary() function to evaluate the results. Calculate the MSE.
```{r}
# create linear model on train.data
train.linearModel <- lm(xPredictor ~ yResponse, data=train.data)
print(train.linearModel)
plot(train.linearModel)
```

```{r}
# summarize the model
train.linearModel.Summary <- summary (train.linearModel)
print(train.linearModel.Summary)
```

```{r}
# calculate MSE
train.Redisuals <- train.linearModel.Summary$residuals
train.Model.MSE <- mean(train.Redisuals^2)
print(train.Model.MSE)

```


```{r}
# evaluate relationship
cor(xPredictor, yResponse)  # calculate correlation between horsepower and MPG
```

#3.	(No code) Write in the white text area: 

#a.	Write the equation from the model  

# Equation:
# yi = 194.2123 - 3.8688xi + N(0, (25.12)squared)

# Notes:
# yi=B0+B1*xi+ei
# where e-N(0, (o)squared)
# B0 is the Estimate value in the (Intercept) row (specifically, 194.2123)
# B1 is the Estimate value in the x row (specifically, -3.8688)
# o is the Residual standard error (specifically, 25.12)


#b.	Is there a strong relationship between horsepower and mpg? 

# The correlation between horsepower and mpg is -0.7690193, this shows a strong negative relationship
# Meaning as horsepower becomes greater, the mpg drops lower

#c.	Is it a positive or negative correlation? 

# This is a negative correlation.

#d.	Comment on the RSE, Rsquared, and F-statistic 

#e.	Comment on the MSE

# The MSE quanitifies the amount of error in the model.
# The MSE is calculated from squaring the mean of the residuals.
# This MSE is 626.6729, and we will use this value later to compare the models



```{r}

# data analysis for understanding and different types of plots
# http://r-statistics.co/Linear-Regression.html

# # scatterplot for data view
# xPredictor <- train.data$horsepower
# yResponse <- train.data$mpg
# scatter.smooth(x=xPredictor, yResponse, main="MPG ~ Horsepower", xlab="Horsepower", ylab="Mpg")  # scatterplot
# 
# 
# # box plot for outliers, anything between 25% and 75% percentiles
# par(mfrow=c(1, 2))  # divide graph area in 2 columns
# boxplot(xPredictor, main="horsepower", sub=paste("Outlier rows: ", boxplot.stats(xPredictor)$out))  # box plot for 'speed'
# boxplot(yResponse, main="MPG", sub=paste("Outlier rows: ", boxplot.stats(yResponse)$out))  # box plot for 'distance'
# 
# 
# if(!require("e107")){
# # install auto.dataFrame if needed
#   #install.packages("e1071", dep = TRUE)
#   library(e1071)
# }
# 
# # Density plot
# par(mfrow=c(1, 2))  # divide graph area in 2 columns
# plot(density(xPredictor), main="Density Plot: Speed", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(xPredictor), 2)))  # density plot for 'speed'
# polygon(density(xPredictor), col="red")
# plot(density(yResponse), main="Density Plot: Distance", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(yResponse), 2)))  # density plot for 'dist'
# polygon(density(yResponse), col="red")
# 
# 
# # correlation
# cor(xPredictor, yResponse)  # calculate correlation between speed and distance 
# 
# # create linear model on train.data
# train.linearModel <- lm(xPredictor ~ yResponse, data=train.data)
# print(train.linearModel)
# plot(train.linearModel)
# # create summary
# summary(train.linearModel)
# 
# 
# #Step 2: Develop the model on the train.dataing data and use it to predict the distance on test.data data
# # Predict from the linearModel of the train.dataing and the train.dataing data
# train.LinearModel.prediction <- predict(train.linearModel, train.data)  # predict distance
# 
# 
# AIC (train.linearModel)  # Calculate akaike information criterion
# 
# 
# # calculate MSE
# #https://stats.stackexchange.com/questions/107643/how-to-get-the-value-of-mean-squared-error-in-a-linear-regression-in-r
# train.Model.MSE <- mean(train.linearModel$residuals^2)
# print(train.Model.MSE)
```



#4.	Plot train.data$mpg~train.data$horsepower and draw a blue abline(). Comment on how well the data fits the line. Predict mpg for horsepower of 98. Comment on the predicted value given the graph you created.



#5.	test.data on the test.data data using the predict function. Find the correlation between the predicted values and the mpg values in the test.data data. Comment on the results. Calculate the mse on the test.data results. Compare this to the mse for the train.dataing data. 



#6.	Plot the linear model in a 2x2 arrangement. Do you see evidence of non-linearity from the residuals?



#7.	Create a second linear model with log(mpg) predicted by horsepower. Compare the summary statistic R^2 of the two models. 



#8.	Plot the function with an abline(). Comment on how well the line fits the data.



#9.	Predict on the test.data data using lm2. Find the correlation of the predictions and log() of test.data mpg. Compare this correlation with the correlation you got for lm1. Calculate the MSE for the test.data data on lm2 and compare to lm1.



#10.	Plot the second linear model in a 2x2 arrangement. How does it compare to the first set of graphs?




# Multiple Linear Regression
#1.	Produce a scatterplot matrix which includes all the variables in the data set using the command "pairs(Auto)". List any possible correlations that you observe, listing positive and negative correlations separately, with at least 3 in each category.



#2.	Compute the matrix of correlations between the variables using function cor(). You should exclude the "name" variable since is it qualitative. Write the two strongest positive correlations and their values in the text area. Write the two strongest negative correlations and their values in the text area.



#3.	Convert the origin variable to a factor. Use the lm() function to perform multiple linear regression with mpg as the response and all other variables except name as predictors. Use the summary() function to print the results. Which predictors appear to have a statistically significant relationship to the response?



#4.	Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Are there any leverage points? Display a row from the data set that seems to be a leverage point. 



#5.	Use the * and + symbols to fit linear regression models with interaction effects, choosing whatever variables you think might get better results than your model in step 3 above. Compare the summaries of the two models, particularly R^2. Run anova() on the two models to see if your second model outperformed the previous one. 
