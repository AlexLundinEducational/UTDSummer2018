Name: Alex Lundin
Assignment: HW2

# Linear Regression
#1.	Load library ISLR. Use the names() function and the summary() function to learn more about the Auto auto_dataFrame set. Divide the auto_dataFrame randomly into train_data and test_data sets, with 75% train_data_ Use seed 1234 for reproducibility.
```{r}


# load auto_dataFrame
if(!require('ISLR')){
# install auto_dataFrame if needed
  install.packages('ISLR')  
  library('ISLR')
}

# learn about Auto
names(Auto)
summary(Auto)
auto_linearModel = lm(horsepower~mpg, data=Auto)

# store Auto into auto_dataFrame
auto_dataFrame <- (Auto)
auto_numberOfRows <- nrow(auto_dataFrame)

# Set random seed to ensure reproducability of the shuffle.
set.seed(1234)

# shuffle the auto_dataFrame and store into a new auto_dataFrame frame
shuffled_auto_dataFrame <- auto_dataFrame[sample(auto_numberOfRows), ]

# set train_data auto_dataFrame with the train_data indices
train_data_indices <- 1:round(0.75 * auto_numberOfRows)
train_data <- shuffled_auto_dataFrame[train_data_indices, ]
# set test_data auto_dataFrame with the test_data indices
test_data_indices <- (round(0.75 * auto_numberOfRows) + 1):auto_numberOfRows
test_data <- shuffled_auto_dataFrame[test_data_indices, ]
```




#2.	Use the lm() function to perform simple linear regression on the train_data data with mpg as the response and horsepower as the predictor. Use the summary() function to evaluate the results. Calculate the MSE.
```{r}
# train_xPredictor_hp <- train_data$horsepower
# train_yResponse_mpg <- train_data$mpg
# create linear model on train_data
train_linearModel <- lm(horsepower ~ mpg, data = train_data)
print(train_linearModel)
plot(train_linearModel)
```

```{r}
# summarize the model
train_linearModel_Summary <- summary (train_linearModel)
print(train_linearModel_Summary)
```

```{r}
# calculate MSE
train_Redisuals <- train_linearModel_Summary$residuals
train_Model_MSE <- mean(train_Redisuals^2)
print(train_Model_MSE)

```


```{r}
# evaluate relationship
#cor(train_xPredictor_hp, train_yResponse_mpg)  # calculate correlation between horsepower and MPG
cor(train_data$horsepower, train_data$mpg)  # calculate correlation between horsepower and MPG
```

#3.	(No code) Write in the white text area: 

#a.	Write the equation from the model  
# Answer:
# yi = 194.2123 - 3.8688xi + N(0, (25.12)squared)

# Notes:
# yi=B0+B1*xi+ei
# where e-N(0, (o)squared)
# B0 is the Estimate value in the (Intercept) row (specifically, 194.2123)
# B1 is the Estimate value in the x row (specifically, -3.8688)
# o is the Residual standard error (specifically, 25.12)


#b.	Is there a strong relationship between horsepower and mpg? 
# Answer:
# The correlation between horsepower and mpg is -0.7690193, this shows a strong negative relationship
# Meaning as horsepower becomes greater, the mpg drops lower

#c.	Is it a positive or negative correlation? 
# Answer:
# This is a negative correlation.

#d.	Comment on the RSE, Rsquared, and F-statistic 
# Answer:
# The Residual Squared Error measures how far off the model was from the data_
# The RSE takes into account the degrees of freedom.
# This RSE was 25.12 and it is measured in y units, so the model is off by around 25 miles per gallon

# The R squared value is .59
# This shows the predictor, horsepower, has a noteable effect on MPG.
# As R squared values get closer to 1, this means the predictor has a greater effect on the response.

# The F-statistic is 422 and the p value is very small, 2.2 e^-16
# This means we can have high confidence that the predictors are stastically significant in the Model_

#e.	Comment on the MSE
# Answer:
# The MSE quanitifies the amount of error in the Model_
# The MSE is calculated from squaring the mean of the residuals.
# This MSE is 626.6729, and we will use this value later to compare the models




#4.	Plot train_data$mpg~train_data$horsepower and draw a blue abline(). Comment on how well the data fits the line. Predict mpg for horsepower of 98. Comment on the predicted value given the graph you created.

#The blue fits the data fairly well because it uses the linearmodel generated from the data.
#The predicted mpg for 98 horsepower is -184.5295

```{r}
plot(train_data$mpg~train_data$horsepower)
# draws line the fits the linear model of the plot data
abline(lm(train_data$mpg~train_data$horsepower), col="blue")

# Predict from the linearModel using the coefficients from the model and then the predict function

# save coefficients
print("This is a manual prediction using coefficient extraction:")
train_linearModel_coefficients = coefficients(train_linearModel)
xvalue = 98
manual_test_prediction = train_linearModel_coefficients[2] * xvalue + train_linearModel_coefficients[1]
print(manual_test_prediction)

print("This is prediction using prediction function and 98 mpg dummy data:")
# hallucinate test data
# Copy and edit a row of Auto
dummy_Data <- train_data[c(1),]
# new hp values
dummy_Data[1, 4] <- 98

train_Prediction <- predict(train_linearModel, newdata=dummy_Data) 
train_Prediction
```



#5.	test on the test data using the predict function. 
#Find the correlation between the predicted values and the mpg values in the test data Comment on the results. 
#Calculate the mse on the test_data results. Compare this to the mse for the train_data data

There is a -1 correlation between the predicted values and the mpg values

The mse for the train data was: 626.6729
The mse for the test data was: 7695
The test data had alot more error inside it's model than the train data did

```{r}

print("This is prediction function on test data:")
test_Prediction <- predict(train_linearModel, newdata=test_data) 
test_Prediction
```
#Evaluate Results
```{r}
correlation <- cor(test_Prediction, test_data$mpg)
print(paste("correlation: ", correlation))
mse <- mean((test_Prediction - test_data$mpg)^2)
print(paste("mse: ", mse))
rmse <- sqrt(mse)
print(paste("rmse: ", rmse))
```


#6.	Plot the linear model in a 2x2 arrangement. Do you see evidence of non-linearity from the residuals?

Yes, there are non-linearity in the results demonstrated by the mid range values.
The residuals, errors, show a curve in the middle.

```{r}
par(mfrow=c(2,2)) # Change the panel layout to 2 x 2
plot(train_linearModel)
```


#7.	Create a second linear model with log(mpg) predicted by horsepower. Compare the summary statistic R^2 of the two models. 

The r squared value was .59 for the training model vs .67 for this model, so they are close.

```{r}
# create linear model, using log of mpg this time
lm2 <- lm(horsepower ~ log(mpg), data = train_data)
summary(lm2)
```



#8.	Plot the function with an abline(). Comment on how well the line fits the data_

The line fits the data very closely.

```{r}
plot(train_data$horsepower ~ log(train_data$mpg))
# draws line the fits the linear model of the plot data
abline(lm(horsepower ~ log(mpg), data = train_data), col="blue")
```

#9.	Predict on the test_data data using lm2. Find the correlation of the predictions and log() of test_data mpg. Compare this correlation with the correlation you got for lm1. Calculate the MSE for the test_data data on lm2 and compare to lm1

The correlation for the log model is slightly less, but still strongly low at -0.98.
The MSE is 7556 which slightly more than the first model.

```{r}
print("This is prediction function on test data with log model:")
test_Prediction2 <- predict(lm2, newdata=test_data) 
test_Prediction2

correlation <- cor(test_Prediction2, test_data$mpg)
print(paste("correlation: ", correlation))
mse <- mean((test_Prediction2 - test_data$mpg)^2)
print(paste("mse: ", mse))
rmse <- sqrt(mse)
print(paste("rmse: ", rmse))
```



#10.	Plot the second linear model in a 2x2 arrangement. How does it compare to the first set of graphs?

```{r}
par(mfrow=c(2,2)) # Change the panel layout to 2 x 2
plot(lm2)
```



# Multiple Linear Regression
#1.	Produce a scatterplot matrix which includes all the variables in the data set using the command "pairs(Auto)". 
#List any possible correlations that you observe, listing positive and negative correlations separately, with at least 3 in each category.

Positive:
Displacement and weight
Horsepower and weight
Horsepower and acceleration

Negative
mpg and horsepower
mpg and weight
mpg and displacement

```{r}
pairs(Auto)
```



#2.	Compute the matrix of correlations between the variables using function cor(). 
# You should exclude the "name" variable since is it qualitative. 
# Write the two strongest positive correlations and their values in the text area. 
# Write the two strongest negative correlations and their values in the text area.

Positive:
[2,2], mpg~cylinders,  .95
[2,3], mpg~ displacement, .95


Negative:
[5,7] weight~acceleration, -.83
[4,1] horsepower~mpg, -.83


```{r}
mpgVScyl <- cor(Auto$mpg,Auto$cylinder)
mpgVSdis <- cor(Auto$mpg,Auto$displacement)
mpgVShp <- cor(Auto$mpg,Auto$horsepower)
mpgVSweight <- cor(Auto$mpg,Auto$weight)
mpgVSacc <- cor(Auto$mpg,Auto$acceleration)
mpgVSyr <- cor(Auto$mpg,Auto$year)
mpgVSorigin <- cor(Auto$mpg,Auto$origin)

cylVSmpg <- cor(Auto$cylinder,Auto$mpg)
cylVSdis <- cor(Auto$cylinder,Auto$displacement)
cylVShp <- cor(Auto$cylinder,Auto$horsepower)
cylVSweight <- cor(Auto$cylinder,Auto$weight)
cylVSacc <- cor(Auto$cylinder,Auto$acceleration)
cylVSyr <- cor(Auto$cylinder,Auto$year)
cylVSorigin <- cor(Auto$cylinder,Auto$origin)

disVSmpg <- cor(Auto$displacement,Auto$mpg)
disVScyl <- cor(Auto$displacement,Auto$cylinder)
disVShp <- cor(Auto$displacement,Auto$horsepower)
disVSweight <- cor(Auto$displacement,Auto$weight)
disVSacc <- cor(Auto$displacement,Auto$acceleration)
disVSyr <- cor(Auto$displacement,Auto$year)
disVSorigin <- cor(Auto$displacement,Auto$origin)

hpVSmpg <- cor(Auto$horsepower,Auto$mpg)
hpVScyl <- cor(Auto$horsepower,Auto$cylinder)
hpVSdis <- cor(Auto$horsepower,Auto$displacement)
hpVSweight <- cor(Auto$horsepower,Auto$weight)
hpVSacc <- cor(Auto$horsepower,Auto$acceleration)
hpVSyr <- cor(Auto$horsepower,Auto$year)
hpVSorigin <- cor(Auto$horsepower,Auto$origin)

wtVSmpg <- cor(Auto$weight,Auto$mpg)
wtVScyl <- cor(Auto$weight,Auto$cylinder)
wtVSdis <- cor(Auto$weight,Auto$displacement)
wtVShp <- cor(Auto$weight,Auto$horsepower)
wtVSacc <- cor(Auto$weight,Auto$acceleration)
wtVSyr <- cor(Auto$weight,Auto$year)
wtVSorigin <- cor(Auto$weight,Auto$origin)

accVSmpg <- cor(Auto$acceleration,Auto$mpg)
accVScyl <- cor(Auto$acceleration,Auto$cylinder)
accVSdis <- cor(Auto$acceleration,Auto$displacement)
accVShp <- cor(Auto$acceleration,Auto$horsepower)
accVSwt <- cor(Auto$acceleration,Auto$weight)
accVSyr <- cor(Auto$acceleration,Auto$year)
accVSorigin <- cor(Auto$acceleration,Auto$origin)

yrVSmpg <- cor(Auto$year,Auto$mpg)
yrVScyl <- cor(Auto$year,Auto$cylinder)
yrVSdis <- cor(Auto$year,Auto$displacement)
yrVShp <- cor(Auto$year,Auto$horsepower)
yrVSwt <- cor(Auto$year,Auto$weight)
yrVSacc <- cor(Auto$year,Auto$acceleration)
yrVSorigin <- cor(Auto$year,Auto$origin)

originVSmpg <- cor(Auto$origin,Auto$mpg)
originVScyl <- cor(Auto$origin,Auto$cylinder)
originVSdis <- cor(Auto$origin,Auto$displacement)
originVShp <- cor(Auto$origin,Auto$horsepower)
originVSwt <- cor(Auto$origin,Auto$weight)
originVSacc <- cor(Auto$origin,Auto$acceleration)
originVSyr <- cor(Auto$origin,Auto$year)

correlation_Matrix = matrix( 
  c(mpgVScyl, mpgVSdis, mpgVShp, mpgVSweight, mpgVSacc, mpgVSyr, mpgVSorigin,
   cylVSmpg, cylVSdis, cylVShp, cylVSweight, cylVSacc, cylVSyr, cylVSorigin,
   disVSmpg, disVScyl, disVShp, disVSweight, disVSacc, disVSyr, disVSorigin,
   hpVSmpg, hpVScyl, hpVSdis, hpVSweight, hpVSacc, hpVSyr, hpVSorigin,
   wtVSmpg, wtVScyl, wtVSdis, wtVShp, wtVSacc, wtVSyr, wtVSorigin,
   accVSmpg, accVScyl, accVSdis, accVShp, accVSwt, accVSyr, accVSorigin,
   yrVSmpg, yrVScyl, yrVSdis, yrVShp, yrVSwt, yrVSacc, yrVSorigin,
   originVSmpg, originVScyl, originVSdis, originVShp, originVSwt, originVSacc, originVSyr
    ), 
  nrow=7, 
  ncol=7)

print(correlation_Matrix) 
```


#3.	
# Convert the origin variable to a factor. 
# Use the lm() function to perform multiple linear regression with mpg as the response and all other variables except name as predictors. 
# Use the summary() function to print the results. 
# Which predictors appear to have a statistically significant relationship to the response?

cylinders and displacement

```{r}

fit <- lm(Auto$mpg ~ Auto$cylinder + Auto$displacement + Auto$horsepower + Auto$weight + Auto$acceleration + Auto$year + Auto$origin, data=Auto)
summary(fit) # show results

```


#4.	
# Use the plot() function to produce diagnostic plots of the linear regression fit. 
# Comment on any problems you see with the fit. Are there any leverage points? 
# Display a row from the data set that seems to be a leverage point. 

There are leverage points causing issues with the plot

```{r}
par(mfrow=c(2,2)) # Change the panel layout to 2 x 2
plot(fit)
```


#5.	
# Use the * and + symbols to fit linear regression models with interaction effects, choosing whatever variables you think might get better results than your model in step 3 above. 
#Compare the summaries of the two models, particularly R^2. Run anova() on the two models to see if your second model outperformed the previous one. 

```{r}

```

