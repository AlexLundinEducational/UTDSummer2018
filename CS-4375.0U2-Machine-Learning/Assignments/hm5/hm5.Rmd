---
output:
  word_document: default
  pdf_document: default
  html_document: default
---

Name: Alex Lundin
Assignment: HM5

Problem 1: Default data 

1.	Set up the data. (5 points)
	a.	Load the ISLR library and attach the Default data set.
	b.	Run the dim() and names() functions on Default
	c.	Use function set.seed(2017) so your results are reproducible.
	d.	Divide the data into 80% for training and the rest for a test set.
```{r}
# a
if(!require('ISLR')){
  install.packages('ISLR')  
  library('ISLR')
}
df_default <- Default
attach(Default)
# b
dim(df_default)
names(df_default)

#c
# separate out the train and test dataframes following a similar naming convention of original frame

# Set random seed to ensure reproducibility of the shuffle.
set.seed(2017)


# shuffle the df_default and store into a new df_default frame
df_default_numberOfRows <- nrow(df_default)
shuf_df_default <- df_default[sample(df_default_numberOfRows), ]

# set train_data df_default with the train_data indices
train_data_indices <- 1:round(0.8 * df_default_numberOfRows)
train_data <- shuf_df_default[train_data_indices, ]
# set test_data df_default with the test_data indices
test_data_indices <- (round(0.8 * df_default_numberOfRows) + 1):df_default_numberOfRows
test_data <- shuf_df_default[test_data_indices, ]

```
		

2.	Logistic regression model (5 points)
	a.	create a logistic regression model on the training data where default is predicted by all other variables
	b.	run a summary of the model
	c.	predict Yes/No on the test data
	d.	compute the accuracy
```{r}
# a
glm0 <- glm(default~., data=train_data, family=binomial)
summary(glm0)

# b
train_glm_sum <- summary(glm0)

# c
probs <- predict(glm0, newdata=test_data, type="response")
pred <- ifelse(probs>0.5, 1, 0)

# d
acc <- mean(pred==test_data$default)
print(paste("accuracy = ", acc))
table(pred, test_data$default) 
```
	
	
3.	Decision tree model (10 points)
	a.	create a decision tree model on the training data
	b.	run a summary of the model
	c.	make predictions for the test set
	d.	compute the accuracy
	e.	print the tree with labels
	f.	display the tree in nested decision form by just using the tree name 
4.	Try Random Forest or Boosting (5 points) and compare your results.

Problem 2: Heart data 

1.	Set up the data. (5 points)
	a.	Download the heart data to your machine from Piazza.
	b.	Load the data into R and attach it
	c.	Remove the “X” column
	d.	Set up train and test sets with 80% for training again using seed 2017
	
```{r}

# a
# store path string for loading csv file
dataPathHomeComputer <- "C:\\Users\\alundi1a\\Desktop\\Screen-Cleaner\\GitHub\\UTDSummer2018\\CS-4375.0U2-Machine-Learning\\Assignments\\hm5\\Heart.csv"

# b
# create the dataframe for heart disease
df_default <- read.table(dataPathHomeComputer)

#c
str(df_default)
names(df_default)
#d
# separate out the train and test dataframes following a similar naming convention of original frame

# Set random seed to ensure reproducibility of the shuffle.
set.seed(2017)


# shuffle the df_default and store into a new df_default frame
df_default_numberOfRows <- nrow(df_default)
shuf_df_default <- df_default[sample(df_default_numberOfRows), ]

# set train_data df_default with the train_data indices
train_data_indices <- 1:round(0.8 * df_default_numberOfRows)
train_data <- shuf_df_default[train_data_indices, ]
# set test_data df_default with the test_data indices
test_data_indices <- (round(0.8 * df_default_numberOfRows) + 1):df_default_numberOfRows
test_data <- shuf_df_default[test_data_indices, ]

```
	
	
	
2.	Logistic regression model (5 points)
	a.	create a logistic regression model on the training data where AHD is predicted by all other variables
	b.	run a summary of the model
	c.	predict Yes/No on the test data
	d.	compute the accuracy
	
	
	
3.	Decision Tree Model (5 points)
	a.	create a decision tree model on the training data
	b.	run a summary of the model
	c.	make predictions for the test set
	d.	compute the accuracy
	
	
	
4.	Display the tree (5 points)
	a.	print the tree with labels
	b.	display the tree in nested decision form by just using the tree name 
	
	
	
5.	Cross validation (10 points)
	a.	create a new tree from the cv.tree() function
	b.	look at the $dev and $size variables by displaying the tree using its name
	c.	plot in a 1x2 format:
		i.	$size and $dev
		ii.	$k and $dev
		
		
		
6.	Prune the tree (10 points)



	a.	create a new pruned tree using best=n where n is the optimal size indicated in step 5
	b.	plot the new pruned tree with labels
	c.	Using the pruned tree, make predictions on the test set
	d.	Compute the accuracy
	
	
	
7.	Try Random Forest or Boosting (5 points) and compare your results


Answer the questions at the bottom of your Rmd file. Upload the Rmd file to TurnItIn. 
Questions: (30 points)
Problem 1
	1. In the logistic regression model, which variables were important and which were not?
	2. What was the accuracy of the logistic regression  model and the decision tree model?
	3. In the tree, why might you have a branch where both branches are No?
	4. Write a simple if/else statement that summarizes the Yes/No values in the decision tree.
	5. Is it a good idea to prune this tree? Why or why not?
Problem 2
	1. Which variables were important (2 or 3 **) in the logistic regression model?
	2. Which variables were used to create the decision tree?
	3. Compare the accuracy of the logistic regression model and the decision tree.
	4. What was the accuracy on the pruned tree?
	5. Which model (logistic regression, decision tree) might be more meaningful to a doctor, and why.
	
	
	



1.	Set up the Auto data:
a.	Load the ISLR package and the Auto data
b.	Determine the median value for mpg
c.	Use the median to create a new column in the data set named mpglevel, which is 1 if mpg>median and otherwise is 0. 
Make sure this variable is a factor. We will use mpglevel as the target (response) variable for the algorithms. 
d.	Use the names() function to verify that your new column is in Auto
e.	create a 75-25 train/test split, using seed 1234 but do not include columns 'name' or 'mpg' in either train or test
```{r}
# a
if(!require('ISLR')){
  install.packages('ISLR')  
  library('ISLR')
}

# b

medianMPG <- median(Auto$mpg)
paste("The median MPG is: ", medianMPG)

# c
Auto$mpglevel <- as.factor(ifelse(Auto$mpg > medianMPG, 1, 0))
str(Auto$mpglevel)

# d
names(Auto)

# e
set.seed(1234)
i <- sample(1:nrow(Auto), 0.75*nrow(Auto))
train <- Auto[i,c(2,3,4,5,6,7,8,10)]
test <- Auto[-i,c(2,3,4,5,6,7,8,10)]

```




2.	Plots 
a.	 Set up a 2x2 graph grid and plot the following pairs of plots 
b.	Plot pair 1: plot horsepower~mpg and weight~mpg, setting colors according to the factor mpglevel, ex: col=(Auto$mpglevel)
c.	Plot pair 2:  plot horsepower~mpglevel and weight~mpglevel 
```{r}
# a
par(mfrow=c(2, 1))  # divide graph area in 2 columns
palette(c("red","black"))

# b
plot(Auto$horsepower~Auto$mpg, col=ifelse(Auto$mpglevel==1, "red", "black"))
plot(Auto$weight~Auto$mpg, col=ifelse(Auto$mpglevel==1, "red", "black"))

# c
plot(Auto$horsepower~Auto$mpglevel)
plot(Auto$weight~Auto$mpglevel)
```





3.	Build a Naïve Bayes model 
a.	build the model on the train set
b.	use the predict() function on the test set
c.	create a table comparing predicted to actual values for mpglevel
d.	calculate the mean accuracy
      0.928571428571429

```{r}

# a
library(e1071)
nb1 <- naiveBayes(mpglevel~., data=train)

# b
p1 <- predict(nb1, newdata=test, type="class")

# c
table(p1, test$mpglevel)

# d
paste("The mean accuracy is:", mean(p1==test$mpglevel))


```





4.	SVM linear kernel 
a.	use the tune() function to perform cross-validation to determine the best value for cost
b.	use the parameter(s) from the previous step to build an svm model with a linear kernel on the train set
c.	use the predict() function on the test set
d.	create a table comparing predicted to actual values for mpglevel
e.	calculate the mean accuracy
      The mean accuracy is: 0.908163265306122
      
```{r}

svm_fit1 <- svm(mpglevel~., data=train, kernel="linear", cost=10, scale=FALSE)

# a
tune_svm1 <- tune(svm, mpglevel~., data=train, kernel="linear",
               ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_svm1)

# b
best_mod1 <- tune_svm1$best.model
summary(best_mod1)

# c
svm_pred <- predict(best_mod1, newdata=test)

# d
table(svm_pred, test$mpglevel)

# e
paste("The mean accuracy is:", mean(svm_pred==test$mpglevel))

```





5.	SVM polynomial kernel 
a.	use the tune() function to perform cross-validation to determine the best values for cost and degree
b.	use the parameter(s) from the previous step to build an svm model with a polynomial kernel on the train set
c.	use the predict() function on the test set
d.	create a table comparing predicted to actual values for mpglevel
e.	calculate the mean accuracy
      The mean accuracy is: 0.918367346938776
```{r}

svm_fit2 <- svm(mpglevel~., data=train, kernel="polynomial", cost=1, degree=1, scale=FALSE)

# a
tune_svm2 <- tune(svm, mpglevel~., data=train, kernel="polynomial",
                               ranges=list(cost=c(0.1,1,10,100,1000),
                degree=c(1,2,3,4,5)))
summary(tune_svm2)

# b
best_mod2 <- tune_svm2$best.model
summary(best_mod2)

# c
svm_pred2 <- predict(best_mod2, newdata=test)

# d
table(svm_pred2, test$mpglevel)

# e
paste("The mean accuracy is:", mean(svm_pred2==test$mpglevel))

```





6.	SVM radial kernel 
a.	use the tune() function to perform cross-validation to determine the best values for cost and gamma
b.	use the parameter(s) from the previous step to build an svm model with a radial kernel on the train set
c.	use the predict() function on the test set
d.	create a table comparing predicted to actual values for mpglevel
e.	calculate the mean accuracy
      The mean accuracy is: 0.928571428571429
```{r}

svm_fit3 <- svm(mpglevel~., data=train, kernel="radial", cost=1, gamma=1, scale=FALSE)

# a
tune_svm3 <- tune(svm, mpglevel~., data=train, kernel="radial",
                ranges=list(cost=c(0.1,1,10,100,1000),
                gamma=c(0.5,1,2,3,4)))
summary(tune_svm3)

# b
best_mod3 <- tune_svm3$best.model
summary(best_mod3)

# c
svm_pred3 <- predict(best_mod3, newdata=test)

# d
table(svm_pred3, test$mpglevel)

# e
paste("The mean accuracy is:", mean(svm_pred3==test$mpglevel))


```





7.	Questions. 
a.	Compare the accuracy results for the 4 models
SVM radial kernel had the highest accuracy at .94
Naïve Bayes model ad an accuracy of .92
SVM polynomial kernel had an accuracy of .918
SVM linear kernel had an accuracy of .908

b.	Discuss the advantages and disadvantages of Naïve Bayes versus svm

Naïve Bayes 
   will assume all factors are independent of eachother
   will have high variance and low bias
   Adv
      works well with small data.
   Dis
      this model will miss relations between factors
 
SVM  
   the low value for SVM linear kernel shows we do not have linearly seperable data, but rather radial grouped data
   Adv
      ability to tune hyperparameters to achieve a better fit
      highly versatile, options for kernel type
   Dis
      the tuned model might not always be better