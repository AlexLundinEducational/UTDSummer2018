---
output:
  word_document: default
  pdf_document: default
  html_document: default
---

Name: Alex Lundin
Assignment: HM4




1.	Set up the Auto data:
a.	Load the ISLR package and the Auto data
b.	Determine the median value for mpg
c.	Use the median to create a new column in the data set named mpglevel, which is 1 if mpg>median and otherwise is 0. 
Make sure this variable is a factor. We will use mpglevel as the target (response) variable for the algorithms. 
d.	Use the names() function to verify that your new column is in Auto
e.	create a 75-25 train/test split, using seed 1234 but do not include columns 'name' or 'mpg' in either train or test
```{r}
# a
if(!require('ISLR')){
  install.packages('ISLR')  
  library('ISLR')
}

# b

medianMPG <- median(Auto$mpg)
paste("The median MPG is: ", medianMPG)

# c
Auto$mpglevel <- as.factor(ifelse(Auto$mpg > medianMPG, 1, 0))
str(Auto$mpglevel)

# d
names(Auto)

# e
set.seed(1234)
i <- sample(1:nrow(Auto), 0.75*nrow(Auto))
train <- Auto[i,c(2,3,4,5,6,7,8,10)]
test <- Auto[-i,c(2,3,4,5,6,7,8,10)]

```




2.	Plots 
a.	 Set up a 2x2 graph grid and plot the following pairs of plots 
b.	Plot pair 1: plot horsepower~mpg and weight~mpg, setting colors according to the factor mpglevel, ex: col=(Auto$mpglevel)
c.	Plot pair 2:  plot horsepower~mpglevel and weight~mpglevel 
```{r}
# a
par(mfrow=c(2, 1))  # divide graph area in 2 columns
palette(c("red","black"))

# b
plot(Auto$horsepower~Auto$mpg, col=ifelse(Auto$mpglevel==1, "red", "black"))
plot(Auto$weight~Auto$mpg, col=ifelse(Auto$mpglevel==1, "red", "black"))

# c
plot(Auto$horsepower~Auto$mpglevel)
plot(Auto$weight~Auto$mpglevel)
```





3.	Build a Naïve Bayes model 
a.	build the model on the train set
b.	use the predict() function on the test set
c.	create a table comparing predicted to actual values for mpglevel
d.	calculate the mean accuracy
      0.928571428571429

```{r}

# a
library(e1071)
nb1 <- naiveBayes(mpglevel~., data=train)

# b
p1 <- predict(nb1, newdata=test, type="class")

# c
table(p1, test$mpglevel)

# d
paste("The mean accuracy is:", mean(p1==test$mpglevel))


```





4.	SVM linear kernel 
a.	use the tune() function to perform cross-validation to determine the best value for cost
b.	use the parameter(s) from the previous step to build an svm model with a linear kernel on the train set
c.	use the predict() function on the test set
d.	create a table comparing predicted to actual values for mpglevel
e.	calculate the mean accuracy
      The mean accuracy is: 0.908163265306122
      
```{r}

svm_fit1 <- svm(mpglevel~., data=train, kernel="linear", cost=10, scale=FALSE)

# a
tune_svm1 <- tune(svm, mpglevel~., data=train, kernel="linear",
               ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_svm1)

# b
best_mod1 <- tune_svm1$best.model
summary(best_mod1)

# c
svm_pred <- predict(best_mod1, newdata=test)

# d
table(svm_pred, test$mpglevel)

# e
paste("The mean accuracy is:", mean(svm_pred==test$mpglevel))

```





5.	SVM polynomial kernel 
a.	use the tune() function to perform cross-validation to determine the best values for cost and degree
b.	use the parameter(s) from the previous step to build an svm model with a polynomial kernel on the train set
c.	use the predict() function on the test set
d.	create a table comparing predicted to actual values for mpglevel
e.	calculate the mean accuracy
      The mean accuracy is: 0.918367346938776
```{r}

svm_fit2 <- svm(mpglevel~., data=train, kernel="polynomial", cost=1, degree=1, scale=FALSE)

# a
tune_svm2 <- tune(svm, mpglevel~., data=train, kernel="polynomial",
                               ranges=list(cost=c(0.1,1,10,100,1000),
                degree=c(1,2,3,4,5)))
summary(tune_svm2)

# b
best_mod2 <- tune_svm2$best.model
summary(best_mod2)

# c
svm_pred2 <- predict(best_mod2, newdata=test)

# d
table(svm_pred2, test$mpglevel)

# e
paste("The mean accuracy is:", mean(svm_pred2==test$mpglevel))

```





6.	SVM radial kernel 
a.	use the tune() function to perform cross-validation to determine the best values for cost and gamma
b.	use the parameter(s) from the previous step to build an svm model with a radial kernel on the train set
c.	use the predict() function on the test set
d.	create a table comparing predicted to actual values for mpglevel
e.	calculate the mean accuracy
      The mean accuracy is: 0.928571428571429
```{r}

svm_fit3 <- svm(mpglevel~., data=train, kernel="radial", cost=1, gamma=1, scale=FALSE)

# a
tune_svm3 <- tune(svm, mpglevel~., data=train, kernel="radial",
                ranges=list(cost=c(0.1,1,10,100,1000),
                gamma=c(0.5,1,2,3,4)))
summary(tune_svm3)

# b
best_mod3 <- tune_svm3$best.model
summary(best_mod3)

# c
svm_pred3 <- predict(best_mod3, newdata=test)

# d
table(svm_pred3, test$mpglevel)

# e
paste("The mean accuracy is:", mean(svm_pred3==test$mpglevel))


```





7.	Questions. 
a.	Compare the accuracy results for the 4 models
SVM radial kernel had the highest accuracy at .94
Naïve Bayes model ad an accuracy of .92
SVM polynomial kernel had an accuracy of .918
SVM linear kernel had an accuracy of .908

b.	Discuss the advantages and disadvantages of Naïve Bayes versus svm

Naïve Bayes 
   will assume all factors are independent of eachother
   will have high variance and low bias
   Adv
      works well with small data.
   Dis
      this model will miss relations between factors
 
SVM  
   the low value for SVM linear kernel shows we do not have linearly seperable data, but rather radial grouped data
   Adv
      ability to tune hyperparameters to achieve a better fit
      highly versatile, options for kernel type
   Dis
      the tuned model might not always be better