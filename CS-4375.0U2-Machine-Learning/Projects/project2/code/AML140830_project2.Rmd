---
output:
  word_document: default
  pdf_document: default
  html_document: default
---

Name: Alex Lundin
Assignment: Project 2
# Header for project

Data URL:
https://www.kaggle.com/koki25ando/22000-scotch-whisky-reviews

Data Overview:

Data Label 			Data Type 					Data Description
	index         int
	name	        factor
	category	    factor
	review.point	int
	price	        factor
	currency	    factor
	description   factor
	myCost        int                 custom column I added

												        
```{r}
# store path string for loading csv file
dataPathHomeComputer <- "C:\\Users\\Alex\\Desktop\\data\\scotch_review.csv"

df_s <- read.csv(dataPathHomeComputer)
```

# investigate the data with names, str, head, summary and cor
# add new column, which is price as a numeric, called cost
```{r}
# show how many rows
paste("There are over 2247 observances, also known as rows.")
str(df_s)

# look at the names of the data frame
nameArray <- names(df_s)

# print a useful message with a compact version of the columns
paste("The names of the columns are:")
paste("Column:", nameArray)

# show first 6 instances of the frame
head(df_s)

# add a new column called win
df_s$myCost <- as.numeric(df_s$price)

print("New Column:")

table(df_s$myCost)

df_s <- df_s[,c(4,8)]

```

# Train and Test	
```{r}
# separate out the train and test dataframes following a similar naming convention of original frame

# Set random seed to ensure reproducibility of the shuffle.
set.seed(1958)


 
# Split data in half, exactly
train_data = df_s[1:(0.5*nrow(df_s)),]
dim(train_data)  # 6 11
test_data = df_s[((0.5*nrow(df_s))+1):((0.5*nrow(df_s)*2)),]
dim(test_data) # 26 11

```




# Summarize the frame

```{r}
# store a summary of the data frame
sm <- summary(df_s)

# print the summary
print(sm)

```
Show correlations between review point and cost to view data

```{r}
if(!require("corrplot")){
  install.packages("corrplot")  
  library("corrplot")
}
library(corrplot)
x <- cor((df_s[,c(1,2)]))
corrplot(x, type="upper", order="hclust")


```




Linear regression model
   Target: myCost
   Predictor: review.point
   
   Evaluation:
      Rsquared = 0.005609
          this means the model itself does not explain variance
      low pvalues for the predictor and target
      low pvalue for the f-stastic
      correlation = -0.8511619

      The data is small, and slightly linear. So Linear regression did the best

# Linear Regression Model
# first model for scotch reviews
```{r}
# create linear model on train_data
train_lm1 <- lm(train_data$myCost~train_data$review.point, data=train_data)

print(train_lm1 )
plot(train_lm1)

# store a summary of the model created from the train data and print it
lm1_sm <- summary(train_lm1)
print(lm1_sm)

# attempt to predict the target y value with the train linear model
pred <- predict(train_lm1, test_data)
print("Correlation of -- Prediction of cost:")
cor(pred, test_data$review.point)
```





Decision Tree Model
   Target: myCost
   Predictors: review.point
   
   Evaluation:
      accuracy    =  0.018
      
# second model for scotch review
# Decision Tree Model
```{r}
if(!require('tree')){
  install.packages('tree')  
  library('tree')
}

# create model
tree_1 <- tree(train_data$myCost~train_data$review.point, data=train_data)

# create predictions
pred <- predict(tree_1, newdata=test_data)

# evaluate model using confusion matrix
confMat <- table(test_data$myCost,pred)
accuracy <- sum(diag(confMat))/sum(confMat)
paste("Accuracy is:", accuracy)

```






SVM linear kernel model
   Target: myCost
   Predictors: all
   
   Evaluation:
      accuracy = 0
      Tunning barley helped the RSME
      I think this data set is poorly suited for SVM
      
# third model for scotch review
# SVM linear kernel 

```{r}
rmse <- function(error)
{
  sqrt(mean(error^2))
}

# create the first linear svm
svm_fit1 <- svm(myCost~., data=train_data, kernel="linear", cost=10, scale=FALSE)


# tuned the model
tune_svm1 <- tune(svm, myCost~., data=train_data, kernel="linear",
               ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_svm1)

# save the best model
best_mod1 <- tune_svm1$best.model
summary(best_mod1)

# tuned predictions
svm_pred <- predict(best_mod1, newdata=test_data)


# table(svm_pred, test_data$myCost)

paste("The mean tuned accuracy is:", mean(svm_pred==test_data$myCost))


# For comparing original to tuned
originalPredicted <- predict(svm_fit1 , test_data)
error <- svm_fit1$residuals  # same as data$Y - predictedY
predictionRMSE <- rmse(error)   # 5.70377
paste("The original RMSE is:", predictionRMSE)

# For comparing original to tuned
tunedPredicted <- predict(best_mod1 , test_data)
error <- best_mod1$residuals  # same as data$Y - predictedY
tunedRMSE <- rmse(error)   # 5.70377
paste("The tuned RMSE is:", tunedRMSE)






 

```

PART 2

Name: Alex Lundin
Assignment: Project 2
# Header for project

Data URL:

https://www.kaggle.com/open-powerlifting/powerlifting-database

Data Overview:

Data Label 			  Data Type 					Data Description
MeetID	          Int
Name	            Factor
Sex	              Factor
Equipment	        Factor
Age	              Double
Division	        Factor
BodyweightKg	    Double
WeightClassKg	    Factor
Squat4Kg	        Double
BestSquatKg	      Double
Bench4Kg	        Double
BestBenchKg	      Double
Deadlift4Kg	      Double
BestDeadliftKg	  Double
TotalKg	          Double
Place	            Factor
Wilks             Double

# Load data												        
```{r}
# store path string for loading csv file
dataPathHomeComputer <- "C:\\Users\\Alex\\Desktop\\data\\openpowerlifting.csv"

df_pl <- read.csv(dataPathHomeComputer)
```


# investigate the data with names, str, head, summary and cor
# add a new column called Win, which is 1 if the contender placed first in the event, 0 otherwise
```{r}
# show how many rows
paste("There are 386414 observances, also known as rows.")
str(df_pl)

# look at the names of the data frame
nameArray <- names(df_pl)

# print a useful message with a compact version of the columns
paste("The names of the columns are:")
paste("Column:", nameArray)

# show first 6 instances of the frame
head(df_pl)

# add a new column called win
df_pl$Win <- ifelse(df_pl$Place == 1, 1, 0)

print("New Column:")

table(df_pl$Win)


```

# Summary
```{r}
# store a summary of the data frame, after addition of new column
sm <- summary(df_pl)

# print the summary
print(sm)
```

# Train and Test	
```{r}

# replace all NA's with the mean of the column
# then scale the mean, randomnly, by a number between 0 and 1

for(i in 1:ncol(df_pl)){
  df_pl[is.na(df_pl[,i]), i] <- mean(df_pl[,i], na.rm = TRUE)*runif(1)
  
}

# Set random seed to ensure reproducibility of the shuffle.
set.seed(1958)

# separate out the train and test dataframes following a similar naming convention of original frame
# I have to Split data exactly in half for some algorithims
# I'm only using a small fraction of the entire data set due to the NA ommisions
train_data = df_pl[1:(0.5*nrow(df_pl)),]
dim(train_data)
test_data = df_pl[((0.5*nrow(df_pl))+1):((0.5*nrow(df_pl)*2)),]
dim(test_data)


train_data_small = df_pl[1:(0.001*nrow(df_pl)),]
test_data_small = df_pl[((0.001*nrow(df_pl))+1):((0.001*nrow(df_pl)*2)),]
dim(train_data_small)
dim(test_data_small)
```

Show correlations to view data

```{r}
if(!require("corrplot")){
  install.packages("corrplot")  
  library("corrplot")
}
library(corrplot)
x <- cor((df_pl[,c(5,7,9,10,11,12,13,14,15,18)]))
corrplot(x, type="upper", order="hclust")


```

Feature selection with rcorr matrix

Bench press is highly correlated with wining the entire competition
.11

```{r}
if(!require("Hmisc")){
  install.packages("Hmisc")  
  library("Hmisc")
}

correlationMatrix <- rcorr(as.matrix(df_pl[,c(5,7,9,10,11,12,13,14,15,18)]))

# access the correlation efficients only, using $r
# column 10 is how every column correlates with winning the meet
correlationWithWin <- correlationMatrix$r[,c(10)]

# sets the column names of the data frame with colnames function
columnNames <- c("Age", "Weight", "Squat", "B.Squat", "Bench", "B.Bench", "Dead", "B.Dead", "TotalKg", "Win")
correlationCoefficients <- correlationMatrix$r[1,c(10)]

for (i in 1:10) {
  name <- columnNames[i]
  value <- correlationWithWin[i]
  print(paste("Current column is:", name))
  print(paste("Current value is:", value))

}
paste("Vector print loop done.")

correlation_df <- data.frame(columnNames, correlationWithWin)

plot(correlation_df)


```


Logistic regression model
   Target: Win
   Predictor: bench press
   
   Evaluation:
      AIC: 264795
      accuracy =  .554617586319336
  
      this model achieved the highest accuracy, I think this model did the best because of the size of the data set.
      The AIC score is high, so the model was penalized for being complex
# first model for power lifting      
```{r}


# logistic regression
glm0 <- glm(train_data$Win~train_data$Bench4Kg, data=train_data, family=binomial)
summary(glm0)

# summarize the model
train_glm_sum <- summary(glm0)

# create prediction off model
probs <- predict(glm0, newdata=test_data)
pred <- ifelse(probs>0.5, 1, 0)

# use accuracy to evaluate
acc <- mean(pred==test_data$Win)
print(paste("accuracy = ", acc))
table(pred, test_data$Win) 

```

Decision Tree Model
   Target: Win
   Predictors: bench press
   
   Evaluation:
      accuracy    =  0.51
      
# second model for powerlifting
```{r}
if(!require('tree')){
  install.packages('tree')  
  library('tree')
}

# create model
tree_1 <- tree(train_data$Win~train_data$Bench4Kg, data=train_data)

# create predictions

pred <- predict(tree_1, newdata=test_data)


# evaluate model
rmse <- sqrt(mean((pred-test_data$Win)^2))
paste("Accuracy is:", rmse)

```



Knn regression model
   Target: Win
   Predictors: age and bench press
   
   Evaluation:
      best k      =  3
      accuracy    =  0.17
      
      Since I had to replace NA's this made the data very similar,
      KNN ended up with too many ties when I only used one or two predictors
      
      Also, my computer took too long to use Knn on the full dataset
      So I subsetted the test and train into smaller sets
      
# third model for powerlifting
```{r}

if(!require("caret")){
  install.packages("caret")  
  library("caret")
}

if(!require("DMwR")){
  install.packages("DMwR")  
  library("DMwR")
}

fit <- knnreg(train_data_small[,c(5,11)],train_data_small[,18],k=1)
predictions <- predict(fit, test_data_small[,c(5,11)])
cor(predictions, test_data_small[,18])
mse <- mean((predictions - test_data_small$Win)^2)
paste("Accuracy is:", mse)

```


# find best k
```{r}
cor_k <- rep(0, 20)
mse_k <- rep(0, 20)
i <- 1
for (k in seq(1, 39, 2)){
  fit_k <- knnreg(train_data_small[,c(5,11)],train_data_small[,18],k=k)
  pred_k <- predict(fit_k, test_data_small[,c(5,11)])
  cor_k[i] <- cor(pred_k, test_data_small[,18])
  mse_k[i] <- mean((pred_k - test_data_small[,18])^2)
  print(paste("k=", k, cor_k[i], mse_k[i]))
  i <- i + 1
}

```

# print data for best K
```{r}
fit_3 <- knnreg(train_data_small[,c(5,11)],train_data_small[,18],k=3)
predictions <- predict(fit_3, test_data_small[,c(5,11)])
mse <- mean((predictions - test_data_small[,18])^2)

paste("Accuracy is:", mse)

```

