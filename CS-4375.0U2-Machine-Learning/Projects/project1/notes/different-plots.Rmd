Name: Alex Lundin
Assignment: HW2

# Linear Regression
#1.	Load library ISLR. Use the names() function and the summary() function to learn more about the Auto auto_dataFrame set. Divide the auto_dataFrame randomly into train_data and test_data sets, with 75% train_data_ Use seed 1234 for reproducibility.
```{r}


# load auto_dataFrame
if(!require('ISLR')){
# install auto_dataFrame if needed
  #install.packages('ISLR')  
  library('ISLR')
}

# learn about Auto
names(Auto)
summary(Auto)


# store Auto into auto_dataFrame
auto_dataFrame <- (Auto)
auto_numberOfRows <- nrow(auto_dataFrame)

# Set random seed to ensure reproducability of the shuffle.
set.seed(1234)

# shuffle the auto_dataFrame and store into a new auto_dataFrame frame
shuffled_auto_dataFrame <- auto_dataFrame[sample(auto_numberOfRows), ]

# set train_data auto_dataFrame with the train_data indices
train_data_indices <- 1:round(0.75 * auto_numberOfRows)
train_data <- shuffled_auto_dataFrame[train_data_indices, ]
# set test_data auto_dataFrame with the test_data indices
test_data_indices <- (round(0.75 * auto_numberOfRows) + 1):auto_numberOfRows
test_data <- shuffled_auto_dataFrame[test_data_indices, ]

```




#2.	Use the lm() function to perform simple linear regression on the train_data data with mpg as the response and horsepower as the predictor. Use the summary() function to evaluate the results. Calculate the MSE.
```{r}
xPredictor_hp <- train_data$horsepower
yResponse_mpg <- train_data$mpg
# create linear model on train_data
train_linearModel <- lm(xPredictor_hp ~ yResponse_mpg, data=train_data)
print(train_linearModel)
plot(train_linearModel)
```

```{r}
# summarize the model
train_linearModel_Summary <- summary (train_linearModel)
print(train_linearModel_Summary)
```

```{r}
# calculate MSE
train_Redisuals <- train_linearModel_Summary$residuals
train_Model_MSE <- mean(train_Redisuals^2)
print(train_Model_MSE)

```


```{r}
# evaluate relationship
cor(xPredictor_hp, yResponse_mpg)  # calculate correlation between horsepower and MPG
```

#3.	(No code) Write in the white text area: 

#a.	Write the equation from the model  
# Answer:
# yi = 194.2123 - 3.8688xi + N(0, (25.12)squared)

# Notes:
# yi=B0+B1*xi+ei
# where e-N(0, (o)squared)
# B0 is the Estimate value in the (Intercept) row (specifically, 194.2123)
# B1 is the Estimate value in the x row (specifically, -3.8688)
# o is the Residual standard error (specifically, 25.12)


#b.	Is there a strong relationship between horsepower and mpg? 
# Answer:
# The correlation between horsepower and mpg is -0.7690193, this shows a strong negative relationship
# Meaning as horsepower becomes greater, the mpg drops lower

#c.	Is it a positive or negative correlation? 
# Answer:
# This is a negative correlation.

#d.	Comment on the RSE, Rsquared, and F-statistic 
# Answer:
# The Residual Squared Error measures how far off the model was from the data_
# The RSE takes into account the degrees of freedom.
# This RSE was 25.12 and it is measured in y units, so the model is off by around 25 miles per gallon

# The R squared value is .59
# This shows the predictor, horsepower, has a noteable effect on MPG.
# As R squared values get closer to 1, this means the predictor has a greater effect on the response.

# The F-statistic is 422 and the p value is very small, 2.2 e^-16
# This means we can have high confidence that the predictors are stastically significant in the Model_

#e.	Comment on the MSE
# Answer:
# The MSE quanitifies the amount of error in the Model_
# The MSE is calculated from squaring the mean of the residuals.
# This MSE is 626.6729, and we will use this value later to compare the models



```{r}

# data analysis for understanding and different types of plots
# http://r-statistics.co/Linear-Regression.html

# # scatterplot for data view
# xPredictor_hp <- train_data$horsepower
# yResponse_mpg <- train_data$mpg
# scatter.smooth(x=xPredictor_hp, yResponse_mpg, main="MPG ~ Horsepower", xlab="Horsepower", ylab="Mpg")  # scatterplot
# 
# 
# # box plot for outliers, anything between 25% and 75% percentiles
# par(mfrow=c(1, 2))  # divide graph area in 2 columns
# boxplot(xPredictor_hp, main="horsepower", sub=paste("Outlier rows: ", boxplot.stats(xPredictor_hp)$out))  # box plot for 'speed'
# boxplot(yResponse_mpg, main="MPG", sub=paste("Outlier rows: ", boxplot.stats(yResponse_mpg)$out))  # box plot for 'distance'
# 
# 
# if(!require("e107")){
# # install auto_dataFrame if needed
#   #install.packages("e1071", dep = TRUE)
#   library(e1071)
# }
# 
# # Density plot
# par(mfrow=c(1, 2))  # divide graph area in 2 columns
# plot(density(xPredictor_hp), main="Density Plot: Speed", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(xPredictor_hp), 2)))  # density plot for 'speed'
# polygon(density(xPredictor_hp), col="red")
# plot(density(yResponse_mpg), main="Density Plot: Distance", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(yResponse_mpg), 2)))  # density plot for 'dist'
# polygon(density(yResponse_mpg), col="red")
# 
# 
# # correlation
# cor(xPredictor_hp, yResponse_mpg)  # calculate correlation between speed and distance 
# 
# # create linear model on train_data
# train_linearModel <- lm(xPredictor_hp ~ yResponse_mpg, data=train_data)
# print(train_linearModel)
# plot(train_linearModel)
# # create summary
# summary(train_linearModel)
# 
# 
# #Step 2: Develop the model on the train_dataing data and use it to predict the distance on test_data data
# # Predict from the linearModel of the train_dataing and the train_dataing data
# train_linearModel_prediction <- predict(train_linearModel, train_data)  # predict distance
# 
# 
# AIC (train_linearModel)  # Calculate akaike information criterion
# 
# 
# # calculate MSE
# #https://stats.stackexchange.com/questions/107643/how-to-get-the-value-of-mean-squared-error-in-a-linear-regression-in-r
# train_Model_MSE <- mean(train_linearModel$residuals^2)
# print(train_Model_MSE)
```



#4.	Plot train_data$mpg~train_data$horsepower and draw a blue abline(). Comment on how well the data fits the line. Predict mpg for horsepower of 98. Comment on the predicted value given the graph you created.



#5.	test_data on the test_data data using the predict function. Find the correlation between the predicted values and the mpg values in the test_data data_ Comment on the results. Calculate the mse on the test_data results. Compare this to the mse for the train_dataing data_ 



#6.	Plot the linear model in a 2x2 arrangement. Do you see evidence of non-linearity from the residuals?



#7.	Create a second linear model with log(mpg) predicted by horsepower. Compare the summary statistic R^2 of the two models. 



#8.	Plot the function with an abline(). Comment on how well the line fits the data_



#9.	Predict on the test_data data using lm2. Find the correlation of the predictions and log() of test_data mpg. Compare this correlation with the correlation you got for lm1. Calculate the MSE for the test_data data on lm2 and compare to lm1.



#10.	Plot the second linear model in a 2x2 arrangement. How does it compare to the first set of graphs?




# Multiple Linear Regression
#1.	Produce a scatterplot matrix which includes all the variables in the data set using the command "pairs(Auto)". List any possible correlations that you observe, listing positive and negative correlations separately, with at least 3 in each category.



#2.	Compute the matrix of correlations between the variables using function cor(). You should exclude the "name" variable since is it qualitative. Write the two strongest positive correlations and their values in the text area. Write the two strongest negative correlations and their values in the text area.



#3.	Convert the origin variable to a factor. Use the lm() function to perform multiple linear regression with mpg as the response and all other variables except name as predictors. Use the summary() function to print the results. Which predictors appear to have a statistically significant relationship to the response?



#4.	Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Are there any leverage points? Display a row from the data set that seems to be a leverage point. 



#5.	Use the * and + symbols to fit linear regression models with interaction effects, choosing whatever variables you think might get better results than your model in step 3 above. Compare the summaries of the two models, particularly R^2. Run anova() on the two models to see if your second model outperformed the previous one. 
