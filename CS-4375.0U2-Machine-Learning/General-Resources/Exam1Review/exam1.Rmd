---
title: "R Notebook"
output: html_notebook
---


```{r}
wine <- read.csv("wine_all.csv", header=TRUE)
str(wine)
```



Which columns are quantitative?
All variables except type. These variables are useful for Linear regression.
Which columns are qualitative?
Type. This data is useful for Logistic regression.
What is a factor?
R convention, internally stored as integer but they display as a label.
Divide data into train and test
Why do we set a seed?
To ensure data is exactly reproducible during randomization.
What does sample do?
Randomly extract data from a frame
How did we subset the data?
Why do we divide into train and test sets?
Because some algorithms attempt to memorize the data. So having separate sets will allow use to evaluate the model while still using the same data

```{r}
set.seed(1234)
i <- sample(1:nrow(wine), 0.8*nrow(wine), replace=FALSE)
train <- wine[i,]
test <- wine[-i,]
```


Build a linear regression model
We will try to predict quality from all other factors.

Explain the lm() function
Explain the output of summary including:
What is the formula?
Quality = .007efixed.end + -1.4 . +.09
Is the intercept considered a "predictor"? Why or why not?
What are residuals?
Which predictors seem good, and why?
Low p value, the p value rejects the null hypothesis
What is RSE?
RSE is in units of y
What is R-squared?
What is the F-statistic?
Are these metrics for the train or test set?
What is a dummy variable? Do you see one below?


```{r}
lm1 <- lm(quality~., data=train)
summary(lm1)
```

Plot the Residuals
What do we hope to see? Normal distrubution, as few leverage points as possible
What are outliers? unusual y value
What are leverage points? unusual x value

```{r}
par(mfrow=c(2,2))
plot(lm1)
```


Evaluate on the Test Data
What does pred contain?
What is correlation? Did you get a good correlation? 
how well two vectors track with eachother. 
Indiction of how well the model performs.
What is mse?
How do we compute mse for the test set? For the train set?
Why might rmse be easier to interpret?
In terms of units of y
Look at some predictions vs. actual values.

```{r}
pred <- predict(lm1, newdata=test)
cor_lm <- cor(pred, test$quality)
mse_lm <- mean((pred-test$quality)^2)
mse_train <- mean(lm1$residuals^2)
rmse_lm <- sqrt(mse_lm)
print(cbind(head(pred, n=10), head(test$quality, n=10)))
```

Build another Linear Regression model
What is anova?
analysis of variance
How do we interpret the results?
compare the RSS values, highers is worse, so LM2 was worse
If a linear regression model has predictors with low p-values, should we take them out? Why or why not?
type*varaible is a interaction model
 
```{r}
lm2 <- lm(quality~volatile_acidity+residual_sugar+alcohol+sulphates+type+type*alcohol, data=train)
summary(lm2)
anova(lm1, lm2)
```
 

Linear Regression
supervised or unsupervised?
  supervised
classification or regression?
  regression
parametric or non-parametric?
  parametric
bias (strong assumptions about the shape of the data) and variance?
  high bias
what are some assumptions of the linear model?
  addititve assumption
    predictors are independent
  assuming predictors are not correlated
what is a confounding variable? How can we detect them?
  something correlatres with predictors and the target
how do we model interaction effects?
  *
can a linear model tell us about correlation or causation or both between x and y?
  correlation, lm makes no arguement on causation
does a linear model have to be a straight line? Why or why not?
  no,  we can have polynomial models
  
Build a Logistic Regression Model
We want to predict red/white given all other predictors.

Why do we need "family=binomial"
this sets the fit type
What is a glm?
Generalize linear model
What is the difference in coefficientsbetween in logistic and linear?
coeifficients reprsent change in the log odds for logistic
Can you interpret the coefficients in logistic regression the same as we did for linear regression?
No, they are different, log odds
Explain null deviance v. residual deviance.
Null devicance, deviance just y intercept
residual deviance, deviance with predictors included
What does AIC tell us?
Lowers AIC is better
```{r}
glm1 <- glm(type~., data=train, family=binomial)
summary(glm1)
```



Check the Train and Test
What is this code doing, and why?
Summarize on column or data.
Can accept whole data frame as well.
```{r}
summary(train$type)
##   red white 
##  1284  3913
summary(test$type)
##   red white 
##   315   985
```


Evaluate on the Test Data
What does probs look like?
1's or 2's, you want the majority of data on diagonal
What would it look like without type="response"?
This gives probablities, without type="response", you get likelyhood
How do we translate probabilities into predictions?
ifelse control
How do we know what integers 'type' was coded in?

What does the table show us?
Why do we use accuracy as a metric instead of cor or mse?
because this is classification

```{r}
probs <- predict(glm1, newdata=test, type="response")
pred_glm <- ifelse(probs>0.5, 2, 1)
table(pred_glm, test$type)
##         
## pred_glm red white
##        1 311     3
##        2   4   982
acc_glm <- mean(pred_glm==as.integer(test$type))
```


Logistic Regression
supervised or unsupervised?
supervised
classification or regression?
calssification
parametric or non-parametric?
parametric
bias and variance?
high bias
are the coefficients computed directly or by optimization methods?
optomization

Gradient descent
must have convex function (parabola)
  start somewhere
  look at slope(derivative) by some step sze
  move along slope
  keep going until we hit min slope
  the step size is the parameter alpha


kNN
First try kNN classification with unscaled data.

How did this compare to logistic regression?
What does k mean in kNN?
number of observations for  cluster
kNN Classification
```{r}
library(class)
knn_pred <- knn(train=train[,1:12], test=test[,1:12], cl=train$type, k=3)
table(knn_pred, test$type)
acc_knn <- mean(knn_pred==test$type)
```


Now scale the data and try again.
Almost always, scaling will help
Compare to the previous results.
can use this for classification or regression
Should we try different values of k? Why or why not?
  Classification, use odd k
  Small k, high variance
  Large , high bias
```{r}
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
train_norm <- as.data.frame(lapply(train[,1:12], normalize))
test_norm <- as.data.frame(lapply(test[,1:12], normalize))
knn_pred2 <- knn(train=train_norm, test=test_norm, cl=train$type, k=3)
table(knn_pred2, test$type)

acc_knn_scaled <- mean(knn_pred2==test$type)

```

kNN Regression
Why did we leave out type?
  Type is a factor, and this algorithim doesn't handle factors
```{r}
train_reg <- train_norm[,1:11]
test_reg <- test_norm[,1:11]
library(caret)
fit <- knnreg(train_reg, train$quality, k=3)
predictions <- predict(fit, test_reg)
cor_knn <- cor(predictions, test$quality) 
mse_knn <- mean((predictions - test$quality)^2) 

```



try different values of k
What is the best k?
How does it compare to the other models?
  Linear regression does better on lines
  
  MSE higher for knn, k = 3 linear model ws better

```{r}
test_mse_knn <- rep(0, 40)
test_cor_knn <- rep(0, 40)
for (i in 1:40){
  fit <- knnreg(train_reg, train$quality, k=i)
  pred <- predict(fit, newdata=test_reg)
  test_cor_knn[i] <- cor(pred, test$quality)
  test_mse_knn[i] <- mean((pred - test$quality)^2)
}
which.min(test_mse_knn)
## [1] 22
which.max(test_cor_knn)
## [1] 22
test_mse_knn[22]  # .547
## [1] 0.5470226
test_cor_knn[22]  # .548
## [1] 0.5478941
```


kNN
supervised or unsupervised?
Supervised
classification or regression?
Either
parametric or non-parametric?
non-parametric
bias and variance?
k small variance
k large bias
advantages?
get good results for either classsification or regression on data that doesn't follow a trend
disadvantages?
work to find good k value

```{r}
k-means
set.seed(1234)
df <- wine[]
df <- as.data.frame(scale(df[,-13]))  # remove type
fit.km <- kmeans(df, 2, nstart=20)
fit.km
```


Interpreting clusters
The within-ss for the two clusters are large. This is probably due to the large number of predictors.

Looking at the correlation of the clusters and the type red/white, we seem to have found something in the data.

unsupervised, only learning from data
```{r}
cor(fit.km$cluster, abs(1-as.integer(wine$type)))  # flip 1 and 2
## [1] 0.9517272
Plot
# plot is too dense with all data so randomly select 250
j <- sample(1:nrow(wine), 250, replace=FALSE)
# plot - using pH and alcohold to spread out the points
# color = white/red wine
# shape = cluster identified by kmeans
plot(wine$pH[j], wine$alcohol[j], pch=fit.km$cluster[j], 
     col=c("brown3","goldenrod1")[as.integer(wine$type[j])],
     xlab="alcohol", ylab="pH")
```




# only plotting in 2 dimensions, in 13-d space, white and red wines must be "close"
Try Various K

kmeans redefines what k represents, how many cluster's graph shows.
Rather than how many obs makes up a cluster

```{r}
wsplot <- function(data, nc=15, seed=1234){ 
  wss <- (nrow(data)-1)*sum(apply(data,2,var)) 
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(data,centers=i)$withinss) 
  }
  plot(1:nc, wss, type="b", xlab="Number of Clusters", ylab="Within groups sum of squares")
} 

wsplot(df)  # elbow at 3


fit.km3 <- kmeans(df, 3, nstart=20)
fit.km3
fit.km$withinss
## [1] 20237.60 42240.26
fit.km3$withinss  # lower
## [1] 18770.18 13782.01 18976.59
```


Hierarchical clustering

```{r}
d <- dist(wine)  # data was normalized above
## Warning in dist(wine): NAs introduced by coercion
fit.average <- hclust(d, method="averag")
plot(fit.average, hang=-1, cex=.8)
```

Try fewer examples so we can see the data.
```{r}
j <- sample(1:nrow(wine), 10, replace=FALSE)
d <- dist(wine[j,])  # data was normalized above
## Warning in dist(wine[j, ]): NAs introduced by coercion
fit.small <- hclust(d, method="averag")
plot(fit.small, hang=-1, cex=.8)
```


Compare some values.
```{r}
# compare 2497 and 3532 == -1.9
sum(df[2497,] - df[3532,])
## [1] -1.947792
# compare 3555 and 6127 == 6.84
sum(df[3532,] - df[4341,])  # diff = -1.07, 3 times as much
## [1] 6.841612
```

What is the indication of overfit? 
  Assuming supervised learning
  Model did great on training, but bad on test
  
Underfit
  Model did bad on training
  
Relation of bias and variance with overfitting and underfitting
  high bias underfits, learns big picture ignoring noise
  high variance overfits, learns noise missing big picture
  
Other things to study
  be able to read loss and cost functions and then explain in enhglish
  
  how are loss and cost used in linear regression*, logisitic regression**
  
  matrix notation for data
  
  gradient descent
  
  k-fold cross validation
    k = how many chunks of data
    used when there is only a few items for test data
    divide into chunks
    build model on each chunk
    either 5 or 10 chunks
    calculate accuracy
    average accuracies together
    
  paramter vs hyper parameter
    paramter = data
    hyper parameter = model
    
  kmeans **** 
    k = number of clusters
  hierarchical clusters ****
  
  knn ***
    k = number of neighbors

  
  